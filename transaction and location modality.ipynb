{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "509e0fca-1efb-4a8e-b90f-7f61e5083842",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ab28819-72c6-4ea0-ad7f-828881bf11e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_multimodal_fraud.py\n",
    "# Multimodal fraud detection: location modality + transaction modality + stacking\n",
    "# Fully-ready: auto-download IEEE-CIS data via Kaggle API or auto-generate synthetic fallback.\n",
    "# Notebook-safe argparse: ignores Jupyter's hidden \"-f\" argument.\n",
    "\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "import zipfile\n",
    "import argparse\n",
    "import warnings\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "KAGGLE_COMPETITION = \"ieee-fraud-detection\"  # Kaggle competition slug\n",
    "REQUIRED_FILES = [\"train_transaction.csv\", \"train_identity.csv\"]\n",
    "\n",
    "def pip_install(pkg: str):\n",
    "    try:\n",
    "        __import__(pkg.replace(\"-\", \"_\"))\n",
    "        return True\n",
    "    except Exception:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "def try_download_kaggle_competition(data_dir: str) -> bool:\n",
    "    data_path = Path(data_dir)\n",
    "    data_path.mkdir(parents=True, exist_ok=True)\n",
    "    if all((data_path / f).exists() for f in REQUIRED_FILES):\n",
    "        return True\n",
    "\n",
    "    if not pip_install(\"kaggle\"):\n",
    "        print(\"[WARN] Could not install 'kaggle'; using synthetic fallback.\", flush=True)\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "        api = KaggleApi()\n",
    "        api.authenticate()\n",
    "        api.competition_download_files(KAGGLE_COMPETITION, path=str(data_path), quiet=False)\n",
    "        zip_file = data_path / f\"{KAGGLE_COMPETITION}.zip\"\n",
    "        if zip_file.exists():\n",
    "            with zipfile.ZipFile(zip_file, \"r\") as zf:\n",
    "                zf.extractall(path=data_path)\n",
    "            zip_file.unlink(missing_ok=True)\n",
    "        return all((data_path / f).exists() for f in REQUIRED_FILES)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Kaggle download failed: {e}\", flush=True)\n",
    "        return False\n",
    "\n",
    "def generate_synthetic_ieee(data_dir: str, n=25000, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    TransactionID = np.arange(1, n + 1, dtype=np.int64)\n",
    "    TransactionDT = rng.integers(0, 60 * 60 * 24 * 90, size=n)  # 90 days\n",
    "    TransactionAmt = np.round(rng.lognormal(mean=3.6, sigma=0.75, size=n), 2)\n",
    "    hour = (TransactionDT // 3600) % 24\n",
    "    C1 = rng.poisson(5, size=n)\n",
    "    D1 = rng.exponential(scale=2.5, size=n)\n",
    "    D2 = rng.exponential(scale=3.0, size=n)\n",
    "\n",
    "    regions = np.array([f\"r{i}\" for i in range(1, 151)])\n",
    "    dirichlet_p = rng.dirichlet(alpha=np.ones(len(regions)))\n",
    "    addr1 = rng.choice(regions, size=n, p=dirichlet_p)\n",
    "    addr2 = rng.choice(np.array([\"US\", \"CA\", \"GB\", \"DE\", \"FR\", \"IN\", \"SG\", \"AU\", \"BR\", \"JP\"]), size=n)\n",
    "    P_emaildomain = rng.choice(np.array([\n",
    "        \"gmail.com\",\"yahoo.com\",\"hotmail.com\",\"live.com\",\"outlook.com\",\n",
    "        \"edu.co.uk\",\"example.in\",\"example.de\",\"example.fr\",\"example.jp\"\n",
    "    ]), size=n)\n",
    "    dist1 = np.abs(rng.normal(loc=12, scale=9, size=n))\n",
    "    dist2 = np.abs(rng.normal(loc=6, scale=6, size=n))\n",
    "\n",
    "    amount_risk = (np.log1p(TransactionAmt) - np.log1p(np.median(TransactionAmt)))\n",
    "    offhour = ((hour < 6) | (hour > 22)).astype(int)\n",
    "    rare_region = pd.Series(addr1).map(pd.Series(addr1).value_counts(normalize=True)) < 0.01\n",
    "    foreign = (pd.Series(addr2) != \"US\").astype(int)\n",
    "    tld = pd.Series(P_emaildomain).str.split(\".\").str[-1]\n",
    "    rare_tld = ~tld.isin([\"com\", \"net\", \"org\"]).astype(int)\n",
    "    risk = 0.9*amount_risk + 0.7*offhour + 0.6*(dist1 > 25) + 0.6*(dist2 > 15) + \\\n",
    "           0.5*foreign + 0.5*rare_region.astype(int) + 0.4*rare_tld\n",
    "    prob = 1 / (1 + np.exp(-(-2.2 + risk)))\n",
    "    isFraud = (rng.random(n) < prob).astype(np.int8)\n",
    "\n",
    "    train_trx = pd.DataFrame({\n",
    "        \"TransactionID\": TransactionID,\n",
    "        \"isFraud\": isFraud,\n",
    "        \"TransactionDT\": TransactionDT,\n",
    "        \"TransactionAmt\": TransactionAmt,\n",
    "        \"C1\": C1,\n",
    "        \"D1\": D1,\n",
    "        \"D2\": D2,\n",
    "        \"addr1\": addr1,\n",
    "        \"addr2\": addr2,\n",
    "        \"dist1\": dist1,\n",
    "        \"dist2\": dist2,\n",
    "        \"P_emaildomain\": P_emaildomain\n",
    "    })\n",
    "\n",
    "    DeviceInfo = rng.choice(np.array([\"android\", \"ios\", \"windows\", \"linux\", None]), size=n)\n",
    "    DeviceType = rng.choice(np.array([\"mobile\", \"desktop\", None]), size=n)\n",
    "    id_30 = rng.choice(np.array([\"Windows 10\", \"Windows 7\", \"iOS 13.3\", \"Android 10\", None]), size=n)\n",
    "    id_31 = rng.choice(np.array([\"chrome\", \"safari\", \"firefox\", \"edge\", None]), size=n)\n",
    "\n",
    "    train_id = pd.DataFrame({\n",
    "        \"TransactionID\": TransactionID,\n",
    "        \"DeviceInfo\": DeviceInfo,\n",
    "        \"DeviceType\": DeviceType,\n",
    "        \"id_30\": id_30,\n",
    "        \"id_31\": id_31\n",
    "    })\n",
    "\n",
    "    data_path = Path(data_dir)\n",
    "    data_path.mkdir(parents=True, exist_ok=True)\n",
    "    train_trx.to_csv(data_path / \"train_transaction.csv\", index=False)\n",
    "    train_id.to_csv(data_path / \"train_identity.csv\", index=False)\n",
    "\n",
    "def reduce_mem(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for col in df.select_dtypes(include=[\"int64\", \"int32\"]).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n",
    "    for col in df.select_dtypes(include=[\"float64\", \"float32\"]).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast=\"float\")\n",
    "    return df\n",
    "\n",
    "def engineer_time_features(df: pd.DataFrame):\n",
    "    if \"TransactionDT\" in df.columns:\n",
    "        df[\"DT_day\"] = (df[\"TransactionDT\"] // (24 * 60 * 60)).astype(np.int32)\n",
    "        df[\"DT_hour\"] = ((df[\"TransactionDT\"] // 3600) % 24).astype(np.int16)\n",
    "        df[\"DT_wday\"] = (df[\"DT_day\"] % 7).astype(np.int8)\n",
    "    return df\n",
    "\n",
    "def normalize_device_email(df: pd.DataFrame):\n",
    "    for c in [\"DeviceInfo\", \"DeviceType\", \"id_30\", \"id_31\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].fillna(\"unknown\").astype(str).str.lower()\n",
    "    for c in [\"P_emaildomain\", \"R_emaildomain\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].fillna(\"unknown\").astype(str).str.lower()\n",
    "    for c in [\"P_emaildomain\", \"R_emaildomain\"]:\n",
    "        if c in df.columns:\n",
    "            df[c + \"_tld\"] = df[c].apply(lambda x: x.split(\".\")[-1] if x != \"unknown\" else \"unknown\")\n",
    "    return df\n",
    "\n",
    "def build_features(df: pd.DataFrame):\n",
    "    df = engineer_time_features(df)\n",
    "    df = normalize_device_email(df)\n",
    "    loc_cats = [c for c in [\"addr1\", \"addr2\", \"P_emaildomain_tld\"] if c in df.columns]\n",
    "    loc_nums = [c for c in [\"dist1\", \"dist2\"] if c in df.columns]\n",
    "    trx_nums = [c for c in [\"TransactionAmt\", \"DT_hour\", \"DT_wday\", \"C1\", \"D1\", \"D2\"] if c in df.columns]\n",
    "    return loc_cats, loc_nums, trx_nums\n",
    "\n",
    "def prepare_data(data_dir: str):\n",
    "    data_path = Path(data_dir)\n",
    "    if not all((data_path / f).exists() for f in REQUIRED_FILES):\n",
    "        ok = try_download_kaggle_competition(str(data_path))\n",
    "        if not ok:\n",
    "            print(\"[INFO] Using synthetic IEEE-CIS-like dataset fallback (no manual steps required).\", flush=True)\n",
    "            generate_synthetic_ieee(str(data_path))\n",
    "\n",
    "    train_trx = pd.read_csv(data_path / \"train_transaction.csv\")\n",
    "    train_id = pd.read_csv(data_path / \"train_identity.csv\")\n",
    "    train = train_trx.merge(train_id, on=\"TransactionID\", how=\"left\")\n",
    "    train = reduce_mem(train)\n",
    "\n",
    "    y = train[\"isFraud\"].astype(np.int8)\n",
    "    X = train.drop(columns=[\"isFraud\"])\n",
    "\n",
    "    loc_cats, loc_nums, trx_nums = build_features(X)\n",
    "\n",
    "    for c in loc_nums + trx_nums:\n",
    "        if c in X.columns:\n",
    "            X[c] = X[c].fillna(0.0)\n",
    "    for c in loc_cats:\n",
    "        X[c] = X[c].fillna(\"unknown\").astype(str)\n",
    "    return X, y, loc_cats, loc_nums, trx_nums\n",
    "\n",
    "def build_pipelines(loc_cats, loc_nums, trx_nums):\n",
    "    try:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", min_frequency=0.01, sparse_output=False)\n",
    "    except TypeError:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", min_frequency=0.01, sparse=False)\n",
    "\n",
    "    loc_pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", ohe, loc_cats),\n",
    "            (\"num\", StandardScaler(), loc_nums),\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "    trx_pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", StandardScaler(), trx_nums),\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "    loc_clf = HistGradientBoostingClassifier(\n",
    "        max_depth=8, learning_rate=0.05, max_iter=400, class_weight={0: 1, 1: 8}, random_state=42\n",
    "    )\n",
    "    trx_clf = HistGradientBoostingClassifier(\n",
    "        max_depth=8, learning_rate=0.05, max_iter=400, class_weight={0: 1, 1: 8}, random_state=42\n",
    "    )\n",
    "    loc_pipe = Pipeline([(\"pre\", loc_pre), (\"clf\", loc_clf)])\n",
    "    trx_pipe = Pipeline([(\"pre\", trx_pre), (\"clf\", trx_clf)])\n",
    "    meta_clf = LogisticRegression(max_iter=300)\n",
    "    return loc_pipe, trx_pipe, meta_clf\n",
    "\n",
    "def time_based_split(X: pd.DataFrame, y: pd.Series, test_frac=0.2, seed=42):\n",
    "    if \"TransactionDT\" in X.columns:\n",
    "        order = X[\"TransactionDT\"].rank(method=\"first\")\n",
    "        cutoff = np.quantile(order, 1 - test_frac)\n",
    "        test_mask = order >= cutoff\n",
    "        X_train, X_valid = X[~test_mask].copy(), X[test_mask].copy()\n",
    "        y_train, y_valid = y[~test_mask].copy(), y[test_mask].copy()\n",
    "    else:\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "            X, y, test_size=test_frac, stratify=y, random_state=seed\n",
    "        )\n",
    "    return X_train, X_valid, y_train, y_valid\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dc691e4-cb0c-4eaa-8b05-1baf6ad109ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC (location)    : 0.7252\n",
      "AUC (transaction) : 0.7708\n",
      "AUC (stacked)     : 0.8109\n",
      "Best threshold    : 0.0245\n",
      "Outputs saved in ./outputs/tandl\n",
      "Saved visualizations in ./outputs/tandl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================\n",
    "# NEW: Visualization Function\n",
    "# ============================\n",
    "# FIXED: Visualization Function\n",
    "import argparse\n",
    "\n",
    "# ============================\n",
    "def plot_visualizations(y_valid, loc_valid_p, trx_valid_p, meta_valid_p, best_thr,\n",
    "                        auc_loc, auc_trx, auc_meta, X_valid, loc_pipe, trx_pipe, out_dir):\n",
    "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # --- ROC Curves ---\n",
    "    fpr_loc, tpr_loc, _ = roc_curve(y_valid, loc_valid_p)\n",
    "    fpr_trx, tpr_trx, _ = roc_curve(y_valid, trx_valid_p)\n",
    "    fpr_meta, tpr_meta, _ = roc_curve(y_valid, meta_valid_p)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.plot(fpr_loc, tpr_loc, label=f\"Location AUC={auc_loc:.3f}\")\n",
    "    plt.plot(fpr_trx, tpr_trx, label=f\"Transaction AUC={auc_trx:.3f}\")\n",
    "    plt.plot(fpr_meta, tpr_meta, label=f\"Stacked AUC={auc_meta:.3f}\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curves\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"roc_curves.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # --- Confusion Matrix ---\n",
    "    pred_meta = (meta_valid_p >= best_thr).astype(int)\n",
    "    cm = confusion_matrix(y_valid, pred_meta)\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=[\"Legit\", \"Fraud\"], yticklabels=[\"Legit\", \"Fraud\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix (Stacked)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"confusion_matrix.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # --- Transaction model feature importance ---\n",
    "    trx_clf = trx_pipe.named_steps[\"clf\"]\n",
    "    trx_pre = trx_pipe.named_steps[\"pre\"]\n",
    "    result = permutation_importance(\n",
    "        trx_clf, trx_pre.transform(X_valid), y_valid,\n",
    "        n_repeats=10, random_state=42, n_jobs=-1\n",
    "    )\n",
    "    trx_feat_names = trx_pre.get_feature_names_out()\n",
    "    importances = result.importances_mean\n",
    "\n",
    "    # Sort and pick top 15\n",
    "    idx = np.argsort(importances)[-15:]\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(x=importances[idx], y=trx_feat_names[idx], palette=\"viridis\")\n",
    "    plt.title(\"Top Transaction Features - Permutation Importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"feature_importance_transaction.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# --- Location model feature importance ---\n",
    "    loc_clf = loc_pipe.named_steps[\"clf\"]\n",
    "    loc_pre = loc_pipe.named_steps[\"pre\"]\n",
    "    result_loc = permutation_importance(\n",
    "        loc_clf, loc_pre.transform(X_valid), y_valid,\n",
    "        n_repeats=10, random_state=42, n_jobs=-1\n",
    "    )\n",
    "    loc_feat_names = loc_pre.get_feature_names_out()\n",
    "    importances_loc = result_loc.importances_mean\n",
    "\n",
    "    idx_loc = np.argsort(importances_loc)[-15:]\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(x=importances_loc[idx_loc], y=loc_feat_names[idx_loc], palette=\"magma\")\n",
    "    plt.title(\"Top Location Features - Permutation Importance\")\n",
    "    plt.savefig(os.path.join(out_dir, \"feature_importance_Location.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # --- Fraud vs Non-Fraud Distribution ---\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    sns.countplot(x=y_valid, palette=\"Set2\")\n",
    "    plt.xticks([0, 1], [\"Legit\", \"Fraud\"])\n",
    "    plt.title(\"Fraud vs Legit Distribution (Validation)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"fraud_distribution.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # --- Transaction Amount Distribution ---\n",
    "    if \"TransactionAmt\" in X_valid.columns:\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.histplot(x=X_valid[\"TransactionAmt\"], hue=y_valid, bins=50,\n",
    "                     stat=\"density\", common_norm=False, palette=\"Set1\")\n",
    "        plt.xlim(0, X_valid[\"TransactionAmt\"].quantile(0.95))\n",
    "        plt.title(\"Transaction Amount by Fraud Label\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(out_dir, \"amount_distribution.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    # --- Fraud Rate by Hour of Day ---\n",
    "    if \"DT_hour\" in X_valid.columns:\n",
    "        df_tmp = pd.DataFrame({\"hour\": X_valid[\"DT_hour\"], \"fraud\": y_valid})\n",
    "        fraud_rate = df_tmp.groupby(\"hour\")[\"fraud\"].mean()\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        fraud_rate.plot(kind=\"bar\", color=\"coral\")\n",
    "        plt.ylabel(\"Fraud Rate\")\n",
    "        plt.title(\"Fraud Rate by Hour of Day\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(out_dir, \"fraud_rate_by_hour.png\"))\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def main(data_dir: str, out_dir: str):\n",
    "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "    X, y, loc_cats, loc_nums, trx_nums = prepare_data(data_dir)\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = time_based_split(X, y, test_frac=0.2, seed=42)\n",
    "\n",
    "    loc_pipe, trx_pipe, meta_clf = build_pipelines(loc_cats, loc_nums, trx_nums)\n",
    "    loc_pipe.fit(X_train, y_train)\n",
    "    trx_pipe.fit(X_train, y_train)\n",
    "\n",
    "    loc_valid_p = loc_pipe.predict_proba(X_valid)[:, 1]\n",
    "    trx_valid_p = trx_pipe.predict_proba(X_valid)[:, 1]\n",
    "    meta_X_valid = np.vstack([loc_valid_p, trx_valid_p]).T\n",
    "    meta_clf.fit(meta_X_valid, y_valid)\n",
    "\n",
    "    auc_loc = roc_auc_score(y_valid, loc_valid_p)\n",
    "    auc_trx = roc_auc_score(y_valid, trx_valid_p)\n",
    "    meta_valid_p = meta_clf.predict_proba(meta_X_valid)[:, 1]\n",
    "    auc_meta = roc_auc_score(y_valid, meta_valid_p)\n",
    "\n",
    "    fpr, tpr, thr = roc_curve(y_valid, meta_valid_p)\n",
    "    best_thr = thr[np.argmax(tpr - fpr)]\n",
    "\n",
    "    with open(os.path.join(out_dir, \"metrics.txt\"), \"w\") as f:\n",
    "        f.write(f\"AUC_location={auc_loc:.5f}\\n\")\n",
    "        f.write(f\"AUC_transaction={auc_trx:.5f}\\n\")\n",
    "        f.write(f\"AUC_stacked={auc_meta:.5f}\\n\")\n",
    "        f.write(f\"best_threshold={best_thr:.6f}\\n\")\n",
    "\n",
    "    pred_df = pd.DataFrame({\n",
    "        \"TransactionID\": X_valid.get(\"TransactionID\", pd.Series(range(len(X_valid)))),\n",
    "        \"proba_location\": loc_valid_p,\n",
    "        \"proba_transaction\": trx_valid_p,\n",
    "        \"proba_stacked\": meta_valid_p,\n",
    "    })\n",
    "    pred_df[\"pred_stacked\"] = (pred_df[\"proba_stacked\"] >= best_thr).astype(int)\n",
    "    pred_df[\"isFraud_true\"] = y_valid.values\n",
    "    pred_df.to_csv(os.path.join(out_dir, \"valid_predictions.csv\"), index=False)\n",
    "\n",
    "    print(f\"AUC (location)    : {auc_loc:.4f}\")\n",
    "    print(f\"AUC (transaction) : {auc_trx:.4f}\")\n",
    "    print(f\"AUC (stacked)     : {auc_meta:.4f}\")\n",
    "    print(f\"Best threshold    : {best_thr:.4f}\")\n",
    "    print(f\"Outputs saved in {out_dir}\")\n",
    "\n",
    "    # === Visualization step ===\n",
    "    plot_visualizations(\n",
    "        y_valid, loc_valid_p, trx_valid_p, meta_valid_p, best_thr,\n",
    "        auc_loc, auc_trx, auc_meta, X_valid, loc_pipe, trx_pipe, out_dir\n",
    "    )\n",
    "    print(f\"Saved visualizations in {out_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data_dir\", type=str, default=\"./data\")\n",
    "    parser.add_argument(\"--out_dir\", type=str, default=\"./outputs/tandl\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "    main(args.data_dir, args.out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7357629f-12d0-4ae9-b4ef-6a5f2e083d63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
